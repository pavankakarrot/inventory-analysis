{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5d26025-b9f6-4a71-b1eb-8883a5bb8df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pavankakarrot/Documents/data_analysis/Inventory_Analysis\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e17ca8-ac45-4ea4-8058-72dc0c0187ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading datasets\n",
    "purchase_prices = pd.read_csv('xxxxxxxx/Inventory_Analysis/2017PurchasePricesDec.csv')\n",
    "beg_inv = pd.read_csv('xxxxxxxx/BegInvFINAL12312016.csv')\n",
    "end_inv = pd.read_csv('xxxxxxxx/Inventory_Analysis/EndInvFINAL12312016.csv')\n",
    "invoice_purchases = pd.read_csv('xxxxxxxx/Inventory_Analysis/InvoicePurchases12312016.csv')\n",
    "purchases = pd.read_csv('xxxxxxxx/Inventory_Analysis/PurchasesFINAL12312016.csv')\n",
    "sales = pd.read_csv('/xxxxxxx/Inventory_Analysis/SalesFINAL12312016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71f32689-2976-42e0-9954-f503d4da5b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purchase_prices Columns:\n",
      "['Brand', 'Description', 'Price', 'Size', 'Volume', 'Classification', 'PurchasePrice', 'VendorNumber', 'VendorName']\n",
      "\n",
      "beg_inv Columns:\n",
      "['InventoryId', 'Store', 'City', 'Brand', 'Description', 'Size', 'onHand', 'Price', 'startDate']\n",
      "\n",
      "end_inv Columns:\n",
      "['InventoryId', 'Store', 'City', 'Brand', 'Description', 'Size', 'onHand', 'Price', 'endDate']\n",
      "\n",
      "invoice_purchases Columns:\n",
      "['VendorNumber', 'VendorName', 'InvoiceDate', 'PONumber', 'PODate', 'PayDate', 'Quantity', 'Dollars', 'Freight', 'Approval']\n",
      "\n",
      "purchases Columns:\n",
      "['InventoryId', 'Store', 'Brand', 'Description', 'Size', 'VendorNumber', 'VendorName', 'PONumber', 'PODate', 'ReceivingDate', 'InvoiceDate', 'PayDate', 'PurchasePrice', 'Quantity', 'Dollars', 'Classification']\n",
      "\n",
      "sales Columns:\n",
      "['InventoryId', 'Store', 'Brand', 'Description', 'Size', 'SalesQuantity', 'SalesDollars', 'SalesPrice', 'SalesDate', 'Volume', 'Classification', 'ExciseTax', 'VendorNo', 'VendorName']\n"
     ]
    }
   ],
   "source": [
    "print(\"purchase_prices Columns:\")\n",
    "print(purchase_prices.columns.tolist())\n",
    "\n",
    "print(\"\\nbeg_inv Columns:\")\n",
    "print(beg_inv.columns.tolist())\n",
    "\n",
    "print(\"\\nend_inv Columns:\")\n",
    "print(end_inv.columns.tolist())\n",
    "\n",
    "print(\"\\ninvoice_purchases Columns:\")\n",
    "print(invoice_purchases.columns.tolist())\n",
    "\n",
    "print(\"\\npurchases Columns:\")\n",
    "print(purchases.columns.tolist())\n",
    "\n",
    "print(\"\\nsales Columns:\")\n",
    "print(sales.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bfc9f8b-ee8d-4d91-9c12-69dde748abea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in purchase_prices:\n",
      "Description    1\n",
      "Size           1\n",
      "Volume         1\n",
      "dtype: int64\n",
      "\n",
      "Missing values in end_inv:\n",
      "City    1284\n",
      "dtype: int64\n",
      "\n",
      "Missing values in invoice_purchases:\n",
      "Approval    5169\n",
      "dtype: int64\n",
      "\n",
      "Missing values in purchases:\n",
      "Size    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing data in each dataset\n",
    "datasets = [purchase_prices, beg_inv, end_inv, invoice_purchases, purchases, sales]\n",
    "dataset_names = [\"purchase_prices\", \"beg_inv\", \"end_inv\", \"invoice_purchases\", \"purchases\", \"sales\"]\n",
    "\n",
    "for name, data in zip(dataset_names, datasets):\n",
    "    missing_values = data.isnull().sum()\n",
    "    non_zero_missing_values = missing_values[missing_values > 0]\n",
    "    \n",
    "    if not non_zero_missing_values.empty:\n",
    "        print(f\"\\nMissing values in {name}:\")\n",
    "        print(non_zero_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c667dee0-d0c7-4068-878c-846e48c404d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No missing values in purchase_prices.\n",
      "\n",
      "No missing values in beg_inv.\n",
      "\n",
      "No missing values in end_inv.\n",
      "\n",
      "No missing values in invoice_purchases.\n",
      "\n",
      "No missing values in purchases.\n",
      "\n",
      "No missing values in sales.\n",
      "Successfully saved purchase_prices to cleaned_data/purchase_prices_cleaned_20241231_100428.csv\n",
      "Successfully saved beg_inv to cleaned_data/beg_inv_cleaned_20241231_100428.csv\n",
      "Successfully saved end_inv to cleaned_data/end_inv_cleaned_20241231_100428.csv\n",
      "Successfully saved invoice_purchases to cleaned_data/invoice_purchases_cleaned_20241231_100428.csv\n",
      "Successfully saved purchases to cleaned_data/purchases_cleaned_20241231_100428.csv\n",
      "Successfully saved sales to cleaned_data/sales_cleaned_20241231_100428.csv\n",
      "\n",
      "Cleaning summary saved to cleaned_data/cleaning_summary_20241231_100428.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Your existing missing value handling code\n",
    "# Handling missing values for purchase_prices dataset\n",
    "cols_to_check = ['Description', 'Size', 'Volume']\n",
    "for col in cols_to_check:\n",
    "    purchase_prices = purchase_prices[purchase_prices[col].notna()]\n",
    "\n",
    "# Handling missing values for end_inv dataset\n",
    "if end_inv['Store'].nunique() == end_inv['City'].nunique():\n",
    "    city_store_mapping = end_inv[['Store', 'City']].drop_duplicates().set_index('Store').to_dict()['City']\n",
    "    end_inv['City'] = end_inv['City'].fillna(end_inv['Store'].map(city_store_mapping))\n",
    "else:\n",
    "    end_inv['City'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Handling missing values for invoice_purchases dataset\n",
    "invoice_purchases['Approval'].fillna('Pending', inplace=True)\n",
    "\n",
    "# Handling missing values for purchases dataset\n",
    "purchases = purchases[purchases['Size'].notna()]\n",
    "\n",
    "# Check missing values in all datasets\n",
    "datasets = [purchase_prices, beg_inv, end_inv, invoice_purchases, purchases, sales]\n",
    "dataset_names = [\"purchase_prices\", \"beg_inv\", \"end_inv\", \"invoice_purchases\", \"purchases\", \"sales\"]\n",
    "\n",
    "for name, data in zip(dataset_names, datasets):\n",
    "    missing_values = data.isnull().sum()\n",
    "    non_zero_missing_values = missing_values[missing_values > 0]\n",
    "    \n",
    "    if not non_zero_missing_values.empty:\n",
    "        print(f\"\\nMissing values in {name}:\")\n",
    "        print(non_zero_missing_values)\n",
    "    else:\n",
    "        print(f\"\\nNo missing values in {name}.\")\n",
    "\n",
    "# Save cleaned datasets\n",
    "output_directory = 'cleaned_data/'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Save each dataset with timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save datasets in CSV format only\n",
    "for name, data in zip(dataset_names, datasets):\n",
    "    try:\n",
    "        csv_filename = f\"{output_directory}{name}_cleaned_{timestamp}.csv\"\n",
    "        data.to_csv(csv_filename, index=False)\n",
    "        print(f\"Successfully saved {name} to {csv_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {name}: {str(e)}\")\n",
    "\n",
    "# Create a summary report\n",
    "with open(f\"{output_directory}cleaning_summary_{timestamp}.txt\", 'w') as f:\n",
    "    f.write(f\"Data Cleaning Summary - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    for name, data in zip(dataset_names, datasets):\n",
    "        f.write(f\"\\n{name.upper()}\\n\")\n",
    "        f.write(f\"Rows: {data.shape[0]}\\n\")\n",
    "        f.write(f\"Columns: {data.shape[1]}\\n\")\n",
    "        missing = data.isnull().sum()\n",
    "        if missing.any():\n",
    "            f.write(\"Remaining missing values:\\n\")\n",
    "            for col, count in missing[missing > 0].items():\n",
    "                f.write(f\"- {col}: {count}\\n\")\n",
    "        else:\n",
    "            f.write(\"No missing values\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(f\"\\nCleaning summary saved to {output_directory}cleaning_summary_{timestamp}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d4a84c-2300-43c7-9db3-538ee3cd7ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
